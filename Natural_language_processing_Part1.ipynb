{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98e0e816-56c0-492c-9b36-7093c20d7213",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44d58cd5-78ba-4d51-9fb1-16ce47730e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d5bb45d-5686-4d73-b8fa-39f705299844",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "55c5c553-890a-4295-98d8-0155cea3fccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8c251150-1afd-495c-b9b7-e02e733fc1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2747c501-469e-43e4-8655-cb5cc6d2672a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer, RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a49a2de7-72f3-4677-a3d7-00416c914034",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0231285-0630-4f89-a770-2fd812711442",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\keert\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "602ea031-4bd7-4509-ac2c-567e5d1cae6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\keert\\nltk_data\\tokenizers\\punkt\n"
     ]
    }
   ],
   "source": [
    "print(nltk.data.find('tokenizers/punkt'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9aca2948-12f3-406e-a8a8-8538b1de05e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 10.1 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 3.7/12.8 MB 9.5 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 8.9 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 6.8/12.8 MB 8.2 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 8.9/12.8 MB 8.4 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.7/12.8 MB 8.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.3/12.8 MB 8.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 7.9 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1ebcb1-1781-481f-a641-fbe9aa2b4c88",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b75f1da7-3019-4276-aa68-4edaf3be39f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rhymes = \"\"\"Twinkle, twinkle, little star,  \n",
    "How I wonder what you are!  \n",
    "Up above the world so high,  \n",
    "Like a diamond in the sky.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ee6875-04ad-4d35-98b4-137ac4533704",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fbc3b9e-453c-4e95-b8ca-0a550925f164",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(rhymes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84d9c381-f0b7-46b1-83ab-bd77b2a7129d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Twinkle',\n",
       " ',',\n",
       " 'twinkle',\n",
       " ',',\n",
       " 'little',\n",
       " 'star',\n",
       " ',',\n",
       " 'How',\n",
       " 'I',\n",
       " 'wonder',\n",
       " 'what',\n",
       " 'you',\n",
       " 'are',\n",
       " '!',\n",
       " 'Up',\n",
       " 'above',\n",
       " 'the',\n",
       " 'world',\n",
       " 'so',\n",
       " 'high',\n",
       " ',',\n",
       " 'Like',\n",
       " 'a',\n",
       " 'diamond',\n",
       " 'in',\n",
       " 'the',\n",
       " 'sky',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0972e2e-9109-4b3e-a715-0228c276cc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized words:\n",
      "\n",
      "Twinkle\n",
      ",\n",
      "twinkle\n",
      ",\n",
      "little\n",
      "star\n",
      ",\n",
      "\n",
      "\n",
      "How\n",
      "I\n",
      "wonder\n",
      "what\n",
      "you\n",
      "are\n",
      "!\n",
      "\n",
      "\n",
      "Up\n",
      "above\n",
      "the\n",
      "world\n",
      "so\n",
      "high\n",
      ",\n",
      "\n",
      "\n",
      "Like\n",
      "a\n",
      "diamond\n",
      "in\n",
      "the\n",
      "sky\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# First stanza text\n",
    "text = \"\"\"Twinkle, twinkle, little star,\n",
    "How I wonder what you are!\n",
    "Up above the world so high,\n",
    "Like a diamond in the sky.\"\"\"\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print tokens\n",
    "print(\"Tokenized words:\\n\")\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b721618-731c-4377-9d70-df97711ce24a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3eaf20f-9faf-4280-a7a3-c8d88e508da6",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7afb84d0-2870-401a-8b76-5de964b9ac5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stemmers\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "lancaster = LancasterStemmer()\n",
    "regex_stemmer = RegexpStemmer('ing$|s$|e$', min=4)  # Remove endings like 'ing', 's', 'e' from words longer than 4 chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8fbb345f-8b97-4ebb-8d09-ddf2dbf10bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ['Twinkle', ',', 'twinkle', ',', 'little', 'star', ',', 'How', 'I', 'wonder', 'what', 'you', 'are', '!', 'Up', 'above', 'the', 'world', 'so', 'high', ',', 'Like', 'a', 'diamond', 'in', 'the', 'sky', '.']\n",
      "Porter stems: ['twinkl', ',', 'twinkl', ',', 'littl', 'star', ',', 'how', 'i', 'wonder', 'what', 'you', 'are', '!', 'up', 'abov', 'the', 'world', 'so', 'high', ',', 'like', 'a', 'diamond', 'in', 'the', 'sky', '.']\n",
      "Snowball stems: ['twinkl', ',', 'twinkl', ',', 'littl', 'star', ',', 'how', 'i', 'wonder', 'what', 'you', 'are', '!', 'up', 'abov', 'the', 'world', 'so', 'high', ',', 'like', 'a', 'diamond', 'in', 'the', 'sky', '.']\n",
      "Lancaster stems: ['twinkl', ',', 'twinkl', ',', 'littl', 'star', ',', 'how', 'i', 'wond', 'what', 'you', 'ar', '!', 'up', 'abov', 'the', 'world', 'so', 'high', ',', 'lik', 'a', 'diamond', 'in', 'the', 'sky', '.']\n",
      "Regex stems: ['Twinkl', ',', 'twinkl', ',', 'littl', 'star', ',', 'How', 'I', 'wonder', 'what', 'you', 'are', '!', 'Up', 'abov', 'the', 'world', 'so', 'high', ',', 'Lik', 'a', 'diamond', 'in', 'the', 'sky', '.']\n"
     ]
    }
   ],
   "source": [
    "# Stem with each stemmer\n",
    "porter_stems = [porter.stem(word) for word in tokens]\n",
    "snowball_stems = [snowball.stem(word) for word in tokens]\n",
    "lancaster_stems = [lancaster.stem(word) for word in tokens]\n",
    "regex_stems = [regex_stemmer.stem(word) for word in tokens]\n",
    "\n",
    "print(\"Original:\", tokens)\n",
    "print(\"Porter stems:\", porter_stems)\n",
    "print(\"Snowball stems:\", snowball_stems)\n",
    "print(\"Lancaster stems:\", lancaster_stems)\n",
    "print(\"Regex stems:\", regex_stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff812442-671b-4c85-8714-dafc6b051ea3",
   "metadata": {},
   "source": [
    "# Stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ab5102c-9474-4138-9ebf-c9398956cd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7787a231-e2e7-4e38-ad63-f0dd021a8d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3083460-a238-4e5b-96d1-503849f6d223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Twinkle',\n",
       " ',',\n",
       " 'twinkle',\n",
       " ',',\n",
       " 'little',\n",
       " 'star',\n",
       " ',',\n",
       " 'wonder',\n",
       " '!',\n",
       " 'world',\n",
       " 'high',\n",
       " ',',\n",
       " 'Like',\n",
       " 'diamond',\n",
       " 'sky',\n",
       " '.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "71310011-9748-4f39-aa4d-0cf78d5c014a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tokens: ['Twinkle', ',', 'twinkle', ',', 'little', 'star', ',', 'How', 'I', 'wonder', 'what', 'you', 'are', '!', 'Up', 'above', 'the', 'world', 'so', 'high', ',', 'Like', 'a', 'diamond', 'in', 'the', 'sky', '.']\n",
      "After Stopword Removal: ['Twinkle', ',', 'twinkle', ',', 'little', 'star', ',', 'wonder', '!', 'world', 'high', ',', 'Like', 'diamond', 'sky', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Tokens:\", tokens)\n",
    "print(\"After Stopword Removal:\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f02be835-8c59-4603-a71f-9acfb3985110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tokens: ['Twinkle', ',', 'twinkle', ',', 'little', 'star', ',', '\\n', 'How', 'I', 'wonder', 'what', 'you', 'are', '!', '\\n', 'Up', 'above', 'the', 'world', 'so', 'high', ',', '\\n', 'Like', 'a', 'diamond', 'in', 'the', 'sky', '.']\n",
      "After Stopword Removal: ['Twinkle', 'twinkle', 'little', 'star', 'wonder', 'world', 'high', 'Like', 'diamond', 'sky']\n"
     ]
    }
   ],
   "source": [
    "# Filter out stopwords and punctuation\n",
    "filtered_tokens = [token.text for token in doc \n",
    "                   if not token.is_stop and token.is_alpha]\n",
    "\n",
    "print(\"Original Tokens:\", [token.text for token in doc])\n",
    "print(\"After Stopword Removal:\", filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccb517c5-8082-48de-97a7-e3745938485a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tokens: ['Twinkle', 'twinkle', 'little', 'star', ',', 'how', 'I', 'wonder', 'what', 'you', 'are', '.']\n",
      "After Stopword Removal: ['Twinkle', 'twinkle', 'little', 'star', ',', 'wonder', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy's English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text (Twinkle stanza or any text)\n",
    "text = \"Twinkle twinkle little star, how I wonder what you are.\"\n",
    "\n",
    "# Process with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Filter out stopwords and punctuation\n",
    "filtered_tokens = [token.text for token in doc \n",
    "                   if not token.is_stop]\n",
    "\n",
    "print(\"Original Tokens:\", [token.text for token in doc])\n",
    "print(\"After Stopword Removal:\", filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d58b39-30c6-4fcb-810a-43338ed2e0ae",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "270bf5a5-5296-4dd7-a80d-8b2e91a8b550",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecae8c12-a660-4d60-bf58-0a26ccdcc2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_lemmas = [lemmatizer.lemmatize(word) for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28df735e-b88b-4513-9e3e-29dffd00dfb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Twinkle',\n",
       " ',',\n",
       " 'twinkle',\n",
       " ',',\n",
       " 'little',\n",
       " 'star',\n",
       " ',',\n",
       " 'How',\n",
       " 'I',\n",
       " 'wonder',\n",
       " 'what',\n",
       " 'you',\n",
       " 'are',\n",
       " '!',\n",
       " 'Up',\n",
       " 'above',\n",
       " 'the',\n",
       " 'world',\n",
       " 'so',\n",
       " 'high',\n",
       " ',',\n",
       " 'Like',\n",
       " 'a',\n",
       " 'diamond',\n",
       " 'in',\n",
       " 'the',\n",
       " 'sky',\n",
       " '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b007020-1a99-4c84-b068-1d7939e8a2d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'best'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d02ab4e4-eca3-4060-b0f5-0f4a783ffc88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'congragul'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter.stem('congragulate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b010efa0-f6b1-4df2-b0e9-9357262752c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "Twinkle twinkle little star , \n",
      " How I wonder what you are . \n",
      " Up above the world so high , \n",
      " Like a diamond in the sky .\n",
      "\n",
      "Lemmatized Text:\n",
      "twinkle twinkle little star , \n",
      " how I wonder what you be . \n",
      " up above the world so high , \n",
      " like a diamond in the sky .\n"
     ]
    }
   ],
   "source": [
    "# Load English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Input text\n",
    "text = \"\"\"Twinkle twinkle little star,\n",
    "How I wonder what you are.\n",
    "Up above the world so high,\n",
    "Like a diamond in the sky.\"\"\"\n",
    "\n",
    "# Process text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print original and lemmatized versions\n",
    "print(\"Original Text:\")\n",
    "print(\" \".join([token.text for token in doc]))\n",
    "\n",
    "print(\"\\nLemmatized Text:\")\n",
    "print(\" \".join([token.lemma_ for token in doc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddbed43-0f4c-401b-ad05-bbc728d6fb47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20114118-3984-41d3-bc8a-9777ca59b843",
   "metadata": {},
   "source": [
    "# POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c171e6ac-09f7-4ded-8292-bd3862d6f1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\keert/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\keert/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twinkle    --> NNP\n",
      "twinkle    --> VBD\n",
      "little     --> JJ\n",
      "star       --> NN\n",
      ",          --> ,\n",
      "How        --> WRB\n",
      "I          --> PRP\n",
      "wonder     --> VBP\n",
      "what       --> WP\n",
      "you        --> PRP\n",
      "are        --> VBP\n",
      ".          --> .\n",
      "Up         --> IN\n",
      "above      --> IN\n",
      "the        --> DT\n",
      "world      --> NN\n",
      "so         --> RB\n",
      "high       --> JJ\n",
      ",          --> ,\n",
      "Like       --> IN\n",
      "a          --> DT\n",
      "diamond    --> NN\n",
      "in         --> IN\n",
      "the        --> DT\n",
      "sky        --> NN\n",
      ".          --> .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "# Download if not already done\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Input stanza\n",
    "text = \"\"\"Twinkle twinkle little star,\n",
    "How I wonder what you are.\n",
    "Up above the world so high,\n",
    "Like a diamond in the sky.\"\"\"\n",
    "\n",
    "# Tokenize and POS tag\n",
    "tokens = word_tokenize(text)\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# Display result\n",
    "for word, tag in pos_tags:\n",
    "    print(f\"{word:10} --> {tag}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "516abaa7-d3b5-4d11-8688-81b09a60952d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twinkle    --> NOUN       (NN)\n",
      "twinkle    --> NOUN       (NN)\n",
      "little     --> ADJ        (JJ)\n",
      "star       --> NOUN       (NN)\n",
      ",          --> PUNCT      (,)\n",
      "\n",
      "          --> SPACE      (_SP)\n",
      "How        --> SCONJ      (WRB)\n",
      "I          --> PRON       (PRP)\n",
      "wonder     --> VERB       (VBP)\n",
      "what       --> PRON       (WP)\n",
      "you        --> PRON       (PRP)\n",
      "are        --> AUX        (VBP)\n",
      ".          --> PUNCT      (.)\n",
      "\n",
      "          --> SPACE      (_SP)\n",
      "Up         --> ADP        (RP)\n",
      "above      --> ADP        (IN)\n",
      "the        --> DET        (DT)\n",
      "world      --> NOUN       (NN)\n",
      "so         --> ADV        (RB)\n",
      "high       --> ADJ        (JJ)\n",
      ",          --> PUNCT      (,)\n",
      "\n",
      "          --> SPACE      (_SP)\n",
      "Like       --> ADP        (IN)\n",
      "a          --> DET        (DT)\n",
      "diamond    --> NOUN       (NN)\n",
      "in         --> ADP        (IN)\n",
      "the        --> DET        (DT)\n",
      "sky        --> NOUN       (NN)\n",
      ".          --> PUNCT      (.)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Input stanza\n",
    "text = \"\"\"Twinkle twinkle little star,\n",
    "How I wonder what you are.\n",
    "Up above the world so high,\n",
    "Like a diamond in the sky.\"\"\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print POS tags\n",
    "for token in doc:\n",
    "    print(f\"{token.text:10} --> {token.pos_:10} ({token.tag_})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdf543b-68e9-4e3f-8cf3-c93afbc48c97",
   "metadata": {},
   "source": [
    "# Named Entitty Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "212f5a80-4774-4e08-b8b6-02f3ce65691e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pussy --> GPE\n",
      "London --> GPE\n",
      "Pussy --> PERSON\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "\n",
    "# Input text\n",
    "text = \"\"\"\n",
    "Pussy cat, pussy cat, where have you been?\n",
    "I've been to London to visit the Queen.\n",
    "Pussy cat, pussy cat, what did you there?\n",
    "I frightened a little mouse under the chair.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize & POS tag\n",
    "tokens = word_tokenize(text)\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# Named Entity Recognition\n",
    "ne_tree = ne_chunk(pos_tags)\n",
    "\n",
    "# Print Named Entities\n",
    "for subtree in ne_tree:\n",
    "    if hasattr(subtree, 'label'):\n",
    "        print(f\"{' '.join(c[0] for c in subtree)} --> {subtree.label()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2e4e6096-b69c-4152-b754-ac09a1d93018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pussy                --> PERSON\n",
      "London               --> GPE\n",
      "Pussy                --> PERSON\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Rhyme text\n",
    "text = \"\"\"\n",
    "Pussy cat, pussy cat, where have you been?\n",
    "I've been to London to visit the Queen.\n",
    "Pussy cat, pussy cat, what did you there?\n",
    "I frightened a little mouse under the chair.\n",
    "\"\"\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print Named Entities\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text:20} --> {ent.label_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16257219-fd93-4d9f-9c9f-2d5a8b4ba786",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aebcd09-11e1-405e-b103-bcb06fd29394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbf0ea20-cc88-4966-9cb1-4c3a06b29e0f",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
